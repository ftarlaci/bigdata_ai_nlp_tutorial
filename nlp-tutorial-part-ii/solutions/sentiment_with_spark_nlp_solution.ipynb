{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08r3y2HQf0Dj"
   },
   "source": [
    "**Sentiment Analysis with [Spark NLP](https://nlp.johnsnowlabs.com/?gclid=CjwKCAjwr7X4BRA4EiwAUXjbt8SXPLqhOytb-o6ZpGC67FuhfJkiaI3GR2EvdTItYmQXEK2gIRfmlBoCzt8QAvD_BwE)** \n",
    "\n",
    "\n",
    "In this second part of our tutorial, we will use Spark NLP, an industry level open source NLP library. After implementing the preprocessing steps as we did last time with NLTK, we will use the pretrained sentiment_analyzer from Spark NLP to see an example of how to use a pretrained model for sentiment analysis. \n",
    "\n",
    "Our goal is to introduce you to one of the most robust NLP tools and libraries that you can continue learning more about as you keep experimenting with NLP techniques. \n",
    "\n",
    "\n",
    "*Please note, in order to have a full grasp of Spark NLP, as well as any other NLP library or tool, you will first need to get familiarized with their documentation and concepts. To learn more about Spark NLP visit the [documentation](https://nlp.johnsnowlabs.com/docs/en/concepts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxnWejbbf0Dk"
   },
   "source": [
    "+ We will first setup the necessary colab environment. \n",
    "  + Run this block only if you are inside Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0P75ZK8f0Dk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhjgcoR1f0Ds"
   },
   "source": [
    "+ Next, we will mount Google colab by running the cell below and clicking on the URL to get the authorization code. \n",
    "  + If you are coding along, copy and paste your authorization code from the url that appears after you run the cell below to the provided box. If you are using Jupyter Notebook, you don't need to do this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YJ2HNYwf0Ds"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l372akSCf0Dv"
   },
   "source": [
    "+ We have now set up our environment on Google colab and can continue with the next steps, using Spark NLP to do sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkQzQNBxkEA8"
   },
   "source": [
    "### 1. Sentiment Analysis Using the pretrained Pipeline\n",
    "\n",
    "Using a pretrained pipeline with spark dataframes we can also use the pipeline through a spark dataframe. We just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the `.transform()` method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe.\n",
    "\n",
    "In this example, we are not doing any training or using a model that we created, but we simply use Spark NLP, out of the box, to tell us what the sentiment of a text that we give to it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw8WVBuzkDHX"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PFNzn24f0Dp"
   },
   "source": [
    "+ We will now start a spark nlp session, as well as check for versions of both Apache spark and spark NLP. Running this cell without an error means we have installed the necessary packages correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfeOWeGykDFL"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipyp-gM4o0C7"
   },
   "source": [
    "+ Load the predefined pipeline provided in Spark NLP containing all the annotators we need to run a sentiment analysis on a piece of raw text.\n",
    "\n",
    "+ + The next step in the process is to initialize the pretrained model from Spark NLP. For sentiment analysis, we will use the named `analyze_sentiment` for the English language. \n",
    "\n",
    "+ In this example, we can simply use a text that could be provided by a user, a client, or any piece of text that you would like to get the sentiment associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuxtGi9WkDCu"
   },
   "outputs": [],
   "source": [
    "pipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PVQxXMVpAF6"
   },
   "source": [
    "+ Create random list of sentences that you would like the model to analyze. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXPGIsbQkC-M"
   },
   "outputs": [],
   "source": [
    "dataset = [\"Since there is No Vaccine for COVID-19 I have no choice but to wear my mask to protect, my family, myself and others. fact is many people have died from COVID-19 are you willing to take that risk, and possibly even put your family in harms way?\", \"Their is NO Vaccine so wear the MASK!\"]\n",
    "\n",
    "# Alternatively, you can put this tiny data into a spark dataframe\n",
    "# data = spark.createDataFrame([[\"Since there is No Vaccine for COVID-19 I have no choice but to wear my mask to protect, my family, myself and others. fact is many people have died from COVID-19 are you willing to take that risk, and possibly even put your family in harms way?\", \"Their is NO Vaccine so wear the MASK!\"]]).toDF('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gjSZOfbkC73"
   },
   "outputs": [],
   "source": [
    "# Annotate our tiny dataset\n",
    "result = pipeline.annotate(dataset)\n",
    "[(r['sentence'], r['sentiment']) for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzbNs8geqFcy"
   },
   "outputs": [],
   "source": [
    "# We can also view each stage in the pipeline by simply printing it.\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rianiXpLrpHR"
   },
   "source": [
    "### 2. Sentiment Analysis Using A Pretrained Model, SentimentDL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKvDbTlrx4VC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHptwvJWuoQk"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0q7adCNj4xj-"
   },
   "source": [
    "+ Let's pull in our twitter dataset that we used last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTmfS8Wp2hxv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/content/My Drive/Colab Notebooks/nlp-tutorial-part-ii/data/covid19_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIGB-yw44nEw"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPy42SY0XHL0"
   },
   "source": [
    "+ Pick only the relevant columns for the sentiment analysis task to reduce data size. \n",
    "  + pretrained pipelines expect the input column to be named “text”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWhIou0YHW4a"
   },
   "outputs": [],
   "source": [
    "df = df[['tweet', 'sentiment']]\n",
    "df = df.rename(columns={'tweet': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vb_2SmCgHbhZ"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59eKQIurXgZw"
   },
   "source": [
    "+ Split the dataset into train and test sets, save these subsets into two different csv files, using pandas and numpy\n",
    "  + This can also be done with `scikit-learn` library as we did last time. This is simply another way of splitting our data if you are trying to reduce the overhead of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJi6LfW-2h0u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Randomly select %80 of the dataset and use it for training. \n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "trainDataset = df[mask]\n",
    "\n",
    "# Take the complement of the training set we have split above (i.e %20 of the data for testing).\n",
    "testDataset = df[~mask]\n",
    "\n",
    "#save these subsets (train & test) into csv\n",
    "trainDataset.to_csv('/content/content/My Drive/Colab Notebooks/nlp-tutorial-part-ii/data/trainDataset.csv', index=False)\n",
    "testDataset.to_csv('/content/content/My Drive/Colab Notebooks/nlp-tutorial-part-ii/data/testDataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJ67TOkl53yo"
   },
   "source": [
    "+ See how many rows of data we have in training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4UPBbtX8D-U"
   },
   "outputs": [],
   "source": [
    "trainDataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFToT0Pb9GL2"
   },
   "outputs": [],
   "source": [
    "testDataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxy_fNfT8kbo"
   },
   "outputs": [],
   "source": [
    "trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nl59bYA-57mW"
   },
   "source": [
    "+ Convert the data into a pyspark dataframe to make it compatible with Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "che6inNi-Cvx"
   },
   "outputs": [],
   "source": [
    "spark_train = spark.createDataFrame(trainDataset.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEpX3Qy4y7h-"
   },
   "outputs": [],
   "source": [
    "spark_test = spark.createDataFrame(testDataset.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xwBXuHpdn4r"
   },
   "outputs": [],
   "source": [
    "spark_train.show(n=10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVcFo-pS6Pc0"
   },
   "source": [
    "+ Setup the Pipeline for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pusEZ5rLf0EB"
   },
   "source": [
    "+ With any new tool or library libray, there is often some specific terminology that you need to learn. In this case, the term we need to pay attention to is \"pipeline,\"\n",
    "    + *In Machine Learning, a pipeline is often defined as a sequence of algorithms to process and learn from data. It is a sequence of stages, and in Spark NLP, each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. That is, the data are passed through the fitted pipeline in order. For more details on Spark Pipelines that Spark NLP uses, please visit [here](http://spark.apache.org/docs/latest/ml-pipeline.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nz2sVAXP9inl"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JMbSmh19iqT"
   },
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "use = UniversalSentenceEncoder.pretrained() \\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# the classes/labels/categories are in sentiment column\n",
    "sentimentdl = SentimentDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"sentiment\")\\\n",
    "  .setMaxEpochs(3)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        sentimentdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM74r_bU6hJ_"
   },
   "source": [
    "+ Train the model on our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPq2Ar9V9iu5"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(spark_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xRlGJea6SqIA"
   },
   "source": [
    "### Save and load pre-trained SentimentDL model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYzum7o4Sj1e"
   },
   "outputs": [],
   "source": [
    "pipelineModel.stages[-1].write().overwrite().save('./tmp_sentimentdl_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d2uKuhm0nU-"
   },
   "source": [
    "+ Use our pre-trained SentimentDLModel in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_99ySASW6vZm"
   },
   "outputs": [],
   "source": [
    "# In a new pipeline we can load it for prediction\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained() \\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "sentimentdl = SentimentDLModel.load(\"./tmp_sentimentdl_model\") \\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        sentimentdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88kkBKbA4E_T"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "dfTest = spark.createDataFrame([\n",
    "    \"I am glad I read this book on the latest trends in Natural Language Processing.\",\n",
    "    \"This movie is ridiculous. I wish I hadn't come to watch it.\"\n",
    "], StringType()).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67Yc7_K44FDU"
   },
   "outputs": [],
   "source": [
    "prediction = pipeline.fit(dfTest).transform(dfTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UevsNPfC4FG8"
   },
   "outputs": [],
   "source": [
    "prediction.select(\"class.result\").show()\n",
    "\n",
    "prediction.select(\"class.metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UC1i4QhIR2U-"
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULdjRAzvR5fk"
   },
   "source": [
    "Similar to other NLP libraries, we can use the evaluation metrics for NLP, evaluating our Spark NLP sentimentdl model. For this, we will first run the model on our test set. We leave it to you for practice to experiment with evaluations metrics in `scikit-learn` library. (Hint: Revisit Part I notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f6sFBY0SIjc"
   },
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(spark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atoRFBMdabS0"
   },
   "outputs": [],
   "source": [
    "predictions.select('sentiment','text',\"class.result\").show(20, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhA7i86S5ktY"
   },
   "source": [
    "+ SentimentDL has the ability to accept a threshold to set a label on any result that is less than that number. By default the threshold is set on 0.6 and everything below that will be assigned as neutral. You can change this label with `setThresholdLabel` attribute.\n",
    "\n",
    "+ We need to filter neutral results since we don't have any in the original test dataset to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-uTYNhnabYb"
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions.select('sentiment','text',\"class.result\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz6RxXSwpxuc"
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions_df[predictions_df['result'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntwy6EtZ6TLc"
   },
   "outputs": [],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXj_ZNN154NH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#alternatively\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_with_spark_nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
