{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/logo.png\" width=\"20%\"></img>\n",
    "</center>\n",
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>\n",
    "# Table of Contents\n",
    "* [Data Pre-processing](#Data-Pre-processing)\n",
    "\t* [Steps to pre-process data](#Steps-to-pre-process-data)\n",
    "\t\t* [Tokenization](#Tokenization)\n",
    "\t\t* [Remove Stop words](#Remove-Stop-words)\n",
    "\t\t* [Stemming](#Stemming)\n",
    "\t\t* [Word Embedding: Representing Text as Numerical Vectors](#Word-Embedding:-Representing-Text-as-Numerical-Vectors)\n",
    "* [Split the Dataset into Training and Testing Sets](#Split-the-Dataset-into-Training-and-Testing-Sets)\n",
    "* [Selecting a Classifier](#Selecting-a-Classifier)\n",
    "* [Create an Instance of RandomForestClassifier()](#Create-an-Instance-of-RandomForestClassifier%28%29)\n",
    "* [Fit the Model and Predict the Test Set](#Fit-the-Model-and-Predict-the-Test-Set)\n",
    "* [Evaluation of Performance](#Evaluation-of-Performance)\n",
    "* [Bonus material:  lemmatizer exercise](#Bonus-material:--lemmatizer-exercise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1-3 are the typical steps taken to clean and process the data to prepare our features (step 4).\n",
    "\n",
    "1. Tokenize\n",
    "2. Perform stemming/lemmatization\n",
    "3. Remove stop words\n",
    "4. Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we're going to be working with a text loaded in the following cell for all our pre-processing steps.  The python package, Pandas, is a convenient way to read in the data and use it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\".\")\n",
    "data_path = root / \"data\" / \"preprocess_corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_path, sep='\\n', header = None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**: Segmentation of text into words (a form of feature extraction)\n",
    "<div align=\"center\">\n",
    "  <img height = 400, width = 400, src=\"./images/tokenize4.jpg\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several options for tokenizing are available in the NLTK library.  Some tokenizers are more refined than others and may significantly contribute to the results that you gain from your NLP models depending on your purposes and the type of data you have. \n",
    "\n",
    "Two different tokenizers are included in the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example is `word_tokenize` from the nltk.tokenize module.  It simply splits a sentence into a list of words, symbols, and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[4].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import word_tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tokens = word_tokenize(data.iloc[4].text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RegexpTokenizer()` is another useful `nltk` tokenizer.  Characters in a ***regex*** (or regular expression) define a search pattern of a given text. In this example, we are using the `RegexpTokenizer()` to remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the RegexpTokenizer module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create an instance of RegexpTokenizer with your search pattern. \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokens = tokenizer.tokenize(data.iloc[4].text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of words that are not important from the information point of view, such as: the, is, a, etc.\n",
    "The NLTK library has a list of stopwords that can be used to remove words from your corpus.\n",
    "Let's look at the stopwords in English that are defined in nltk library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Remove stop words from our list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: Use either a for loop or a list comprehension to go through the list of tokens, check if its in the \n",
    "#list of stop words, and keep it if it is not.\n",
    "\n",
    "#Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**: Reduces words to their root, but the root might not always result in an actual word.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img height = 300, width = 300, src=\"./images/stem2.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several stemmers available in NLTK.  Some examples are `nltk.PorterStemmer` and the `SnowballStemmer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your stemmer of choice\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "# Create an instance of the PorterStemmer()\n",
    "stemmer = PorterStemmer()  \n",
    "\n",
    "# Note: A word stem need not be the same root as a dictionary-based morphological root, \n",
    "# it just is an equal to or smaller form of the word. \n",
    "for w in tokens_no_sw: \n",
    "    print(w, \" : \", stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In some cases, you may need to do more than stemming.  There is a process called **\"lemmatization\"** that reduces a word to a root in a more sophisticated way.  See the bonus section at the very end of the notebook for examples of a lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run through all of these steps (tokenization, remove stop words, and stemming) on our dataset we are considering today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Tokenize, remove stop words, and stem each line from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the twitter dataset.\n",
    "\n",
    "data = pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at what the data looks like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas library to inspect data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list as our initial corpus.  We will add to it as we pre-process each line.\n",
    "#Note: this cell takes almost 10 minutes to run with this dataset.  \n",
    "corpus = []\n",
    "\n",
    "#loop through each line of data\n",
    "for n in range(len(data)):    \n",
    "    sentence = data['tweet'][n]\n",
    "    \n",
    "    #1. Tokenize\n",
    "    tokens = ...\n",
    "    \n",
    "    #2. remove stop words\n",
    "    tokens_no_sw = ...\n",
    "    \n",
    "    #3. stem the remaining words\n",
    "    stem_words = ...\n",
    "    \n",
    "    \n",
    "    #Join the words back together as one string and append this string to your corpus.\n",
    "    corpus.append(' '.join(stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the above cell takes awhile to complete, I saved the resulting corpus in a pickle file for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('./data/corpus.txt', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding: Representing Text as Numerical Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We first need to represent texts to numbers that the learning algorithm can process. \n",
    "+ To represent each word in the dataset, we will use `CountVectorizer` from `scikit-learn` library. This is a very straightforward class for converting words into features.\n",
    "+ `CountVectorizer` will also lowercase and tokenize the data, but it is good practice to know how to do such preprocessing as we have done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "      <img height = 350, width = 350, src=\"./images/one_hot2.jpg\">\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CountVectorizer module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a vectorizer instance\n",
    "vectorizer = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already pre-processed our data and created a corpus to insert into our CountVectorizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the CountVectorizer is expecting a list of strings as a corpus\n",
    "\n",
    "# The function fit_transform() is used for dataset transformations in scikit-learn. \n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "# In this instance, the dataframe data contains target labels in column  5\n",
    "y = data.iloc[:,5].values\n",
    "\n",
    "#extract the feature names (words) to view in our dataframe\n",
    "labels = vectorizer.get_feature_names()\n",
    "\n",
    "#create a pandas dataframe with the columns being our words (or features)\n",
    "df = pd.DataFrame(data=X, columns=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ To be able to test the accuracy of Machine Learning models, we need to have a set of data that our model has not seen during training. \n",
    "+ To achieve this, we will use the function `train_test_split()` from `sklearn.model_selection` to split the dataset into `train` and `test` sets. \n",
    "+ It is common practice to take only 20% of the total data as the test set. However, depending on the nature of your data, you can play with the ratios to see if a better performance can be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/split.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_test and y_test contains 20% of our data which we reserve for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We will use `RandomForestClassifier()` from `scikit-learn` library as our classifier for this exercise. \n",
    "     + Please note, there are a number of classifiers in `scikit_learn` that you can use for classification problems, such as:\n",
    "     + `AdaBoostClassifier`\n",
    "     + `GaussianProcessClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Instance of RandomForestClassifier() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomForestClassifier()` takes a bunch of parameters. For our purposes, we will specify only a few:\n",
    "+ `n_estimators` is the number of trees in the forest.\n",
    "+ `criterion` determines what a good split for the tree is. We can select `gini` or `entropy` for the model. \n",
    "+ `random_state` controls both the randomness of the bootstrapping of the samples used when building trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators= 100, criterion=\"entropy\", random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model and Predict the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In `scikit-learn`, an estimator for classification is a Python object that implements the methods `fit(X_train, y_train)` and `predict(X_test)`\n",
    "+ We call the methods `fit(X_train, y_train)` and `predict(X_test)` on our `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Fit function adjusts weights according to data values so that better accuracy can be achieved. \n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# model.predict() : given a trained model, predict the label of a new set of data.\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Accuracy** is the total number of correct predictions divided by the total number of predictions. \n",
    "+ This metric might not always be a very good indicator of performance, particularly for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy Score:', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus material:  lemmatizer exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interested in exploring a lemmatizer, see the following examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** -- Lemmatization is a more sophisticated approach to reducing to root words.  See the difference in the results here.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img height = 300, width = 300, src=\"./images/lemma.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['trespassed', 'argued', 'languages', 'rocks', 'radii', 'punctuate', 'car''s', 'ran', 'distanced', 'spoke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(words)):\n",
    "    print(f'{words[n]}: {lemmatizer.lemmatize(words[n])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Notice these did not change some of these values to a root word.  This is because they are verbs and the lemmatizer default is a noun.  You can specify verb in order to get a root word of a verb as well.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('spoke', pos ='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('ran', pos ='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('distanced', pos ='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#TOC\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/logo.png\" width=\"25%\"></img>\n",
    "</center>\n",
    "Copyright Quansight LLC 2018-2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
